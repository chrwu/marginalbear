{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec, Word2Vec\n",
    "import os\n",
    "import collections\n",
    "import smart_open\n",
    "import random\n",
    "import subprocess as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(fname, sample=1, offset=1, delimiter='\\t', drop=False, dropth=0.1):\n",
    "    sample = max(int(sample), 1)\n",
    "    offset = max(int(offset), 0)\n",
    "    with open(fname, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if drop:\n",
    "                if random.random() <= dropth:\n",
    "                    continue\n",
    "                \n",
    "            if (i + offset) % sample == 0:\n",
    "                try:\n",
    "                    tag, sent = line.split(delimiter)\n",
    "                    # For training data, add tags\n",
    "                    yield TaggedDocument(sent.strip().split(), [tag])\n",
    "                except Exception as err:\n",
    "                    print(i, line)\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Doc2Vec(\n",
    "    size=50, window=10,\n",
    "    negative=10,\n",
    "    dm_concat=1, dbow_words=1,\n",
    "    min_count=5, sample=1e-5,\n",
    "    workers=6\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_tokenized = 'dump/doc0909.csv'\n",
    "train_gr = read_corpus(file_tokenized, sample=1, offset=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_gr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec(dm/c,d50,n10,w10,mc5,s1e-05,t6)\n",
      "12869583\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(model.corpus_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 26min 17s, sys: 3min 39s, total: 1h 29min 56s\n",
      "Wall time: 51min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(10):\n",
    "    model.min_alpha = model.alpha\n",
    "    tr = read_corpus(file_tokenized, sample=1, offset=0, drop=True, dropth=0.5)\n",
    "    model.train(tr, total_examples=model.corpus_count, epochs=1)\n",
    "    model.alpha *= 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('title2comment.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ [('吳彥祖', 0.8425476551055908), ('城武', 0.7958974838256836), ('彭于晏', 0.740955650806427)]\n",
      "@ [('臭酸宅', 0.8624711632728577), ('臭宅', 0.8433394432067871), ('魯肥宅', 0.8200834393501282)]\n",
      "@ [('姊姊', 0.8106493949890137), ('姐姐', 0.7968341112136841), ('弟弟', 0.7908051013946533)]\n",
      "@ [('鈕承澤', 0.8874450922012329), ('麥可貝', 0.8463934659957886), ('豆導會', 0.8296818733215332)]\n",
      "@ [('好久不見', 0.7861751317977905), ('午安', 0.7624630928039551), ('您好', 0.7485469579696655)]\n",
      "@ [('阿扁', 0.8524560928344727), ('馬英九', 0.8288743495941162), ('謝長廷', 0.7842897176742554)]\n"
     ]
    }
   ],
   "source": [
    "print('@', model.most_similar(['金城武'], topn=3))\n",
    "print('@', model.most_similar(['肥宅'], topn=3))\n",
    "print('@', model.most_similar(['妹妹'], topn=3))\n",
    "print('@', model.most_similar(['豆導'], topn=3))\n",
    "print('@', model.most_similar(['安安'], topn=3))\n",
    "print('@', model.most_similar(['陳水扁'], topn=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175124 0.9503573775291443\n",
      "175124\t老 甕 牛肉麵\n",
      "175124\t超 難吃\n",
      "165029 0.8852249383926392\n",
      "165029\t剛剛 一碗 半 筋 半 肉牛 麵\n",
      "165029\t我 也 正在 牛肉麵\n",
      "165029\t我 也 吃 牛肉麵\n"
     ]
    }
   ],
   "source": [
    "# sent = '館長 和 朱雪璋 pk 誰 贏'\n",
    "sent = '好吃 的 牛肉麵'\n",
    "inferred_vector = model.infer_vector(sent.split(), steps=100)\n",
    "# print(inferred_vector)\n",
    "most_similar = model.docvecs.most_similar([inferred_vector], topn=2)\n",
    "for tag, score in most_similar:\n",
    "    cmd = 'grep -P \"^{tag}\\t\" /var/local/marginalbear/dump/doc0909.csv'.format(tag=tag)\n",
    "    print(tag, score)\n",
    "    print(sp.run(cmd, shell=True, stdout=sp.PIPE).stdout.decode('utf-8').strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97330552857231734"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent1 = '異形 讓 豆導 來 導 會 怎樣 ？'.split()\n",
    "sent2 = '豆導 拍 異形 會 怎樣 ？'.split()\n",
    "model.docvecs.similarity_unseen_docs(model, sent1, sent2, steps=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
