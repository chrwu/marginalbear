{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gevent\n",
    "import gevent.monkey\n",
    "# gevent.monkey.patch_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils import (\n",
    "    PsqlQuery,\n",
    "    PsqlAbstract,\n",
    "    clean_comment, clean_query,\n",
    "    query2halfwidth\n",
    ")\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from core.ingest import PsqlIngester\n",
    "from configparser import RawConfigParser\n",
    "config_parser = RawConfigParser()\n",
    "config_parser.read('../config.ini')\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "mock_data = {\n",
    "  \"title\": \"失憶了該怎麼辦？\",\n",
    "  \"tag\": \"問卦\",\n",
    "  \"spider\": \"gossiping\",\n",
    "  \"url\": \"https://www.ptt.cc/bbs/Gossiping/M.1502013732.A.344.html\",\n",
    "  \"author\": \"shanpinlo ((((((((((()))))))))))\",\n",
    "  \"push\": \"james732: 得了失億可能對你我都好QQ\\ncake10414: .....\\nZNDL: 哪部？\\nA0091127: 先尻一槍\\nyuugen2: 你被學長肛的經驗之談嗎\\nbenkk41: 甲味\",\n",
    "  \"publish_date\": \"2017-08-06T18:02:09+00:00\"\n",
    "}\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PsqlAbstract.set_database_info(\n",
    "    config_parser.get('global', 'dbuser'),\n",
    "    config_parser.get('global', 'dbname'),\n",
    "    config_parser.get('global', 'dbpassword')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OkbotJLParser(object):\n",
    "    def __init__(self, jlpath):\n",
    "        self.jlpath = jlpath\n",
    "    \n",
    "    def batch_parse(self, batch_size=10):\n",
    "        with open(self.jlpath, 'r') as f:\n",
    "            i = 0\n",
    "            parsed = []\n",
    "            for line in f:\n",
    "                parsed.append(self._parse(line))\n",
    "                i += 1\n",
    "                if i >= batch_size:\n",
    "                    i = 0\n",
    "                    yield [ps for ps in parsed if bool(ps)]\n",
    "                    parsed = []\n",
    "\n",
    "            yield [ps for ps in parsed[:i] if bool(ps)]\n",
    "\n",
    "\n",
    "    def _parse(self, line):\n",
    "        try:\n",
    "            post = json.loads(line)\n",
    "            url = post['url']\n",
    "            post['date'] = datetime.strptime(post['publish_date'], '%Y-%m-%dT%H:%M:%S+00:00')\n",
    "            return post\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "#             logger.warning(e)\n",
    "#             logger.warning('command okbot_ingest, jsonline record faild to parse in, ignored. line: {}'.format(line.encode('utf-8').decode('unicode-escape')))\n",
    "            return {}\n",
    "\n",
    "\n",
    "def subtask(post, vocab_comment):\n",
    "    batch_comment = post['batch_comment']\n",
    "    cmt_bundle = ingester.insert_comment(batch_comment, [post['url']])\n",
    "    vocab = ingester.insert_vocab_ignore_docfreq(batch_comment, tokenized_field='comment_tokenized')\n",
    "    ingester.upsert_vocab2comment(batch_comment, vocab, cmt_bundle)\n",
    "    vocab_comment.extend(vocab)\n",
    "\n",
    "def task(batch_post):\n",
    "    post_url = ingester.upsert_post(batch_post)\n",
    "    vocab_post = ingester.insert_vocab_ignore_docfreq(batch_post)\n",
    "    ingester.upsert_vocab2post(batch_post, vocab_post, post_url)\n",
    "    ingester.insert_title(batch_post, post_url)\n",
    "    vocab_comment = []\n",
    "    jobs = [gevent.spawn(subtask, post, vocab_comment) for post in batch_post]\n",
    "    gevent.joinall(jobs)\n",
    "#     for post in batch_post:\n",
    "#         batch_comment = post['batch_comment']\n",
    "#         cmt_bundle = ingester.insert_comment(batch_comment, [post['url']])\n",
    "#         vocab = ingester.insert_vocab_ignore_docfreq(batch_comment, tokenized_field='comment_tokenized')\n",
    "#         ingester.upsert_vocab2comment(batch_comment, vocab, cmt_bundle)\n",
    "#         vocab_comment.extend(vocab)\n",
    "    vocab_comment = list(set(vocab_comment))\n",
    "    ingester.update_vocab_postfreq(vocab_post)\n",
    "    ingester.update_vocab_commentfreq(vocab_comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = OkbotJLParser('formatted.jl')\n",
    "ingester = PsqlIngester('jieba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time @total: 1.20sec.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "start_ = start\n",
    "jobs = []\n",
    "for idx, batch_post in enumerate(parser.batch_parse(batch_size=1)):\n",
    "    if len(batch_post) > 0:\n",
    "        task(batch_post)\n",
    "#         jobs.append(gevent.spawn(task, batch_post))\n",
    "        if idx % 50 == 3:\n",
    "#             gevent.joinall(jobs)\n",
    "#             jobs = []\n",
    "            break\n",
    "# gevent.joinall(jobs)\n",
    "\n",
    "print('Elapsed time @total: {:.2f}sec.'.format(time.time() - start_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post = {'author': 'forget ()',\n",
    " 'batch_comment': [{'audience': 'eatingshit',\n",
    "   'comment': 'high',\n",
    "   'comment_tokenized': [CustomTok('high', 'eng')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 6},\n",
    "  {'audience': 'janbun999',\n",
    "   'comment': '幫高調',\n",
    "   'comment_tokenized': [CustomTok('幫高', 'v'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 1},\n",
    "  {'audience': 'ducantimmy',\n",
    "   'comment': '高調高調',\n",
    "   'comment_tokenized': [CustomTok('高', 'a'), CustomTok('調高', 'v'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 7},\n",
    "  {'audience': 'ota978',\n",
    "   'comment': '高調',\n",
    "   'comment_tokenized': [CustomTok('高', 'a'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 4},\n",
    "  {'audience': 'ota978',\n",
    "   'comment': '高調',\n",
    "   'comment_tokenized': [CustomTok('高', 'a'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 10},\n",
    "  {'audience': 'tfhkrt',\n",
    "   'comment': '幫高',\n",
    "   'comment_tokenized': [CustomTok('幫高', 'v')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 2},\n",
    "  {'audience': 'kenfu0402',\n",
    "   'comment': '高',\n",
    "   'comment_tokenized': [CustomTok('高', 'a')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 3},\n",
    "  {'audience': 'Paul0041',\n",
    "   'comment': '高調',\n",
    "   'comment_tokenized': [CustomTok('高', 'a'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 13},\n",
    "  {'audience': 'lee16818',\n",
    "   'comment': '高調',\n",
    "   'comment_tokenized': [CustomTok('高', 'a'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 11},\n",
    "  {'audience': 'CIRCAshi',\n",
    "   'comment': '高',\n",
    "   'comment_tokenized': [CustomTok('高', 'a')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 12},\n",
    "  {'audience': 'blackchc',\n",
    "   'comment': '幫高調',\n",
    "   'comment_tokenized': [CustomTok('幫高', 'v'), CustomTok('調', 'vn')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 9},\n",
    "  {'audience': 'quester08',\n",
    "   'comment': '高',\n",
    "   'comment_tokenized': [CustomTok('高', 'a')],\n",
    "   'ctype': 'text',\n",
    "   'floor': 5}],\n",
    " 'comment_cleaned': 'janbun999: 幫高調\\ntfhkrt: 幫高\\nkenfu0402: 高\\nota978: 高調\\nquester08: 高\\neatingshit: high\\nducantimmy: 高調高調\\nblackchc: 幫高調\\nota978: 高調\\nlee16818: 高調\\nCIRCAshi: 高\\nPaul0041: 高調',\n",
    " 'comment_raw': 'janbun999: 幫高調\\ntfhkrt: 幫高\\nkenfu0402: 高\\nota978: 高調\\nquester08: 高\\neatingshit: high\\nducantimmy: 高調\\nducantimmy: 高調\\nblackchc:  幫高調\\nota978: 高調\\nlee16818: 高調\\nCIRCAshi: 高\\nPaul0041: 高調',\n",
    " 'ctype': 'text',\n",
    " 'date': datetime(2017, 8, 6, 9, 5, 9),\n",
    " 'tag': '問卦',\n",
    " 'title_cleaned': '朋友弟弟失蹤,請高屏地區的朋友幫忙',\n",
    " 'title_raw': '朋友弟弟失蹤，請高屏地區的朋友幫忙',\n",
    " 'title_tokenized': [CustomTok('朋友', 'n'),\n",
    "  CustomTok('弟弟', 'n'),\n",
    "  CustomTok('失', 'v'),\n",
    "  CustomTok('蹤', 'zg'),\n",
    "  CustomTok(',', 'x'),\n",
    "  CustomTok('請', 'v'),\n",
    "  CustomTok('高', 'a'),\n",
    "  CustomTok('屏', 'ng'),\n",
    "  CustomTok('地區', 'n'),\n",
    "  CustomTok('的', 'uj'),\n",
    "  CustomTok('朋友', 'n'),\n",
    "  CustomTok('幫忙', 'v')],\n",
    " 'url': 'https://www.ptt.cc/bbs/Gossiping/M.1501981512.A.204.html'}\n",
    "ingester = PsqlIngester('gossiping', 'jieba')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_comment = post['batch_comment']\n",
    "cmt_bundle = ingester.insert_comment(batch_comment, [post['url']])\n",
    "vocab_bundle = ingester.insert_vocab_ignore_docfreq(batch_comment, tokenized_field='comment_tokenized')\n",
    "# ingester.upsert_vocab2comment(batch_comment, vocab, cmt_bundle)\n",
    "\n",
    "qvocab, vschema = ingester.query_vocab(vocab_bundle)\n",
    "qcomment, cmtschema = ingester.query_comment(cmt_bundle)\n",
    "tokenized = [[(k.word, k.flag, 'jieba') for k in p['comment_tokenized']] for p in batch_comment]\n",
    "for vocab in qvocab:\n",
    "    vtuple = (vocab[vschema['word']], vocab[vschema['pos']], vocab[vschema['tokenizer']])\n",
    "#     print(tokenized)\n",
    "    comment_id_with_vocab = [cmt[cmtschema['id']] for idx, cmt in enumerate(qcomment) if vtuple in tokenized[idx]]\n",
    "# comment_id_with_vocab = [cmt[cmtschema['id']] for idx, cmt in enumerate(qcomment) if vtuple in tokenized[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s=list(set(vocab_comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20666,\n",
       " [('報警', 'n', 'jieba'),\n",
       "  ('試過', 'v', 'jieba'),\n",
       "  ('反對', 'v', 'jieba'),\n",
       "  ('精', 'a', 'jieba'),\n",
       "  ('法令', 'n', 'jieba'),\n",
       "  ('錫', 'n', 'jieba'),\n",
       "  ('有罪', 'vn', 'jieba'),\n",
       "  ('巔峰', 'n', 'jieba'),\n",
       "  ('投美', 'vn', 'jieba'),\n",
       "  ('聊', 'v', 'jieba')])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s), s[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
